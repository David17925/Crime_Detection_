{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des bibliothèques utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models\n",
    "import numpy as np\n",
    "import math\n",
    "from DeepSORT.deep_sort_pytorch.utils.parser import get_config\n",
    "from DeepSORT.deep_sort_pytorch.deep_sort import DeepSort\n",
    "from tracker_fonction import tracking\n",
    "from  Yolo_nas_custom_dataset_function import model_import\n",
    "from  Yolo_nas_custom_dataset_function import crime_detection_yolocustom_model\n",
    "from datetime import datetime\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vous pouvez le voir nous utilisons super_gradients. En effet, c'est une API développé par DECI qui ont pu développer leur propre modèle YOLO le YOLO-NAS (Network Architecture Search). Pour le moment il est l'état de l'art des modèles YOLO.\n",
    "Le développement de nouvelles architectures produits par les techniques NAS ont surpassé les architectures conçues par l'homme dans des tâches telles que le traitement du langage naturel, la détection d'objets et la classification d'images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " L'architecture YOLO-NAS offre des performances élevées en termes d'exactitude pour la détection d'objets sans nécessiter des ressources informatiques élevées. La technologie AutoNAC explore un vaste espace d'architecture pour trouver des équilibres entre la latence et le débit, et la robustesse du modèle YOLO-NAS se manifeste dans sa capacité à gérer diverses conditions et à maintenir une précision élevée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Description de l'image](Image_video_notebook\\benchmark2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour nos besoins les modèles de tailles M ont le meilleur rapport vitesse/exactitude.Comme on peut le voir sur le graphique le plus rapide et le plus et le plus précis sont le YOLO V6 et le YOLO nas et pour le peu de différence de latence nous avons plutôt penché pour le YOLO NAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'activation (Heatmap) pour que le modèle ne fonctionne que lorsqu'il y a du mouvement sur l'image et ainsi économiser au maximum les ressources de calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer la heatmap\n",
    "def calculate_activation(frame, prev_frame, activation_threshold=500):\n",
    "    # Convertir les images en niveaux de gris\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculer la différence absolue entre les deux images\n",
    "    diff = cv2.absdiff(gray_frame, gray_prev_frame)\n",
    "\n",
    "    # Appliquer un seuil pour détecter les changements significatifs\n",
    "    _, threshold = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Calculer la somme des pixels pour déterminer l'activation\n",
    "    activation = np.sum(threshold)\n",
    "    return activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation du modèle de détection d'agression et du modèle pre-entrainer coco.\n",
    "Le modèle de YOLO NAS pre-entrainer coco ainsi que celui que nous avons entrainer nous-même sont des modèles de deep Learning.\n",
    "\n",
    "Ces modèles de réseaux de neuronnes sont composés de différentes couches.\n",
    "\n",
    "1) Couches de convolution (CNN) Conv2d :\n",
    "\n",
    "Les couches de convolution sont les composants principaux des réseaux de neurones convolutifs (CNN). Elles sont utilisées pour extraire des caractéristiques des images en appliquant des filtres convolutifs.\n",
    "\n",
    "2) Couches regroupement (pooling) MaxPool2d :\n",
    "\n",
    "Les couches de regroupement(pooling) sont utilisées pour réduire la dimensionnalité spatiale des caractéristiques extraites par les couches de convolution, tout en préservant les caractéristiques les plus importantes. Cela permet de réduire le nombre de paramètres et de calculs nécessaires.\n",
    "\n",
    "\n",
    "3) Couches entièrement connectées (Fully-Connected) Dense:\n",
    "\n",
    "Les couches entièrement connectées sont utilisées pour effectuer la classification finale des objets détectés. Elles sont souvent utilisées à la fin du réseau pour mapper les caractéristiques extraites vers les classes d'objets détectées.\n",
    "Fonctions d'activation :\n",
    "\n",
    "Les fonctions d'activation sont utilisées après chaque couche pour introduire une non-linéarité dans le modèle. Les fonctions d'activation couramment utilisées incluent ReLU (Rectified Linear Activation), Leaky ReLU, et Sigmoid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement de la vidéo.\n",
    "A noté que les images sont redimensionner lors du preprocessing à 640x640 pour la couche d'entrer du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in crime_detection-1 to yolov5pytorch:: 100%|██████████| 360128/360128 [00:16<00:00, 21880.08it/s]\n",
      "Extracting Dataset Version Zip to crime_detection-1 in yolov5pytorch:: 100%|██████████| 17384/17384 [00:39<00:00, 437.67it/s]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    #Charger le modèle\n",
    "    best_model = model_import()\n",
    "    device=torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model=models.get('yolo_nas_m', pretrained_weights=\"coco\").to(device)\n",
    "    # Charger la vidéo\n",
    "    video_path = r'C:\\Users\\davbe\\Crime_detection\\fighting.gif'\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    # Obtenir la largeur et la hauteur des frames\n",
    "    frame_width = int(cap.get(3))  # Width of the frames in the video\n",
    "    frame_height = int(cap.get(4))  # Height of the frames in the video\n",
    "\n",
    "    # Lire la première image pour l'utiliser comme référence\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Erreur de capture vidéo.\")\n",
    "        sys.exit()\n",
    "    # Create VideoWriter object\n",
    "    out = cv2.VideoWriter('output_video_vf3.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (frame_width, frame_height))\n",
    "    while True:\n",
    "        # Lire le frame suivant\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        # Afficher l'image webcam\n",
    "        #cv2.imshow('Webcam', frame)\n",
    "\n",
    "        # Attendre 1 milliseconde et vérifier si l'utilisateur appuie sur la touche 'q' pour quitter\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "        if prev_frame is None:\n",
    "            print(\"Erreur: Image précédente vide.\")\n",
    "            continue  # ou effectuez une autre action en conséquence\n",
    "        # Calculer l'activation\n",
    "        out.write(frame)\n",
    "        activation = calculate_activation(frame, prev_frame)\n",
    "        # Utiliser l'activation pour décider d'activer ou non le modèle de détection\n",
    "        if activation > 500:\n",
    "            print(\"première activation 500 à:\", datetime.now())\n",
    "            while (activation>10):\n",
    "                print(\"deuxième activation 10 à:\", datetime.now())\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    print(\"Erreur de capture vidéo.\")\n",
    "                    sys.exit()\n",
    "                prediction = crime_detection_yolocustom_model(best_model,frame,ret)\n",
    "                out.write(frame)\n",
    "                prediction.save(r'C:\\Users\\davbe\\Crime_detection\\crime_detection\\resultatdeladetection3.jpg')  # Save as .mp4\n",
    "\n",
    "                label = prediction[0].prediction.labels\n",
    "                if len(label) > 0:\n",
    "                    for i in label:\n",
    "                        if i == 1:\n",
    "\n",
    "                            print(\"activation agression à:\", datetime.now())\n",
    "                            out.write(frame)\n",
    "                            print(\"LABEL: \", label)\n",
    "                            index = np.where(label == 1)[0]\n",
    "                            print(\"INDEX: \", index)\n",
    "                            bboxes = prediction[0].prediction.bboxes_xyxy[index]\n",
    "                            print(\"BBOXES: \", bboxes)\n",
    "                            print(\"tracking\")\n",
    "                            tracking(cap,model,bboxes)\n",
    "                            out.write(frame)\n",
    "\n",
    "                else:\n",
    "                    # Mettre à jour l'image de référence\n",
    "                    prev_frame = frame\n",
    "                    activation = calculate_activation(frame, prev_frame)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "print (\"fini\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ce code crée un objet VideoWriter de OpenCV pour écrire le résultat des images dans une vidéo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Couches de détection :\n",
    "\n",
    "Les couches de détection dans le modèle YOLO NAS sont responsables de prédire les boîtes englobantes (bounding boxes) et les scores de confiance pour chaque classe d'objet. Ces couches sont généralement constituées de couches de convolution avec des filtres de taille adaptée pour générer des prédictions de détection à différentes échelles.\n",
    "\n",
    "Une fois que les prédictions des coordonnées des boîtes englobantes sont générées, elles subissent souvent un traitement supplémentaire pour garantir leur validité dans l'image. Cela peut impliquer l'application de filtres pour éliminer les boîtes ayant une faible probabilité de contenir un objet, ainsi que l'utilisation de techniques de post-traitement telles que la suppression des boîtes redondantes et la suppression des boîtes avec un faible score de confiance. Ce traitement est souvent effectué par une technique appelée \"Non-Maximum Suppression\" (NMS).\n",
    "\n",
    "Concernant les prédictions de classification des objets basées sur ces boîtes englobantes, la fonction softmax est utilisée car elle permet de normaliser les scores de classe pour obtenir des probabilités facilitant ainsi l'interprétation des prédictions de classification.\n",
    "\n",
    "En résumé, les couches de détection des modèles YOLO jouent un rôle crucial dans la génération de prédictions précises de boîtes englobantes et de scores de confiance, tandis que la fonction softmax est souvent utilisée pour normaliser les scores de classe et obtenir des probabilités pour chaque classe d'objet détecté."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Description de l'image](Image_video_notebook\\pred_0.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
